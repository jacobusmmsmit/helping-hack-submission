---
title: "Submission"
output: html_notebook
author: Martin, Kitti, Janique
---

## Setup

```{r package imports, message=FALSE}
library(tidyverse)
library(tidymodels)
```

```{r theming}
theme_set(theme_minimal())
```

## Exploratory Analysis

```{r data import, message=FALSE}
imd <- read_csv("data/imd.csv")
ref <- read_csv("data/ref.csv")
```

```{r data aggregation}
ref_agg <- ref %>%
  group_by(lad_code, lad_name, class, category) %>%
  # Note: 2017 Devon & Cornwall Police and Crime data is missing
  summarise(expenditure = mean(expenditure, na.rm = TRUE), .groups = 'drop')
```

## Modelling

```{r transform data}
ref_wide <- ref_agg %>%
  mutate(category = str_remove_all(
    str_replace_all(category, ' ', '_'), '[^\\w ]+'
  )) %>%
  spread(key = category, value = expenditure)

```

```{r combine datasets}
combi <- imd %>%
    left_join(select(ref_wide, -c(lad_name, class)), by = 'lad_code') %>%
    select(-c(lad_code, lad_name)) %>%
    na.omit()
```

```{r split data}
set.seed(123)
combi_split <- initial_split(combi, strata = imd_2019)
combi_train <- training(combi_split)
combi_test <- testing(combi_split)
```

```{r create XGBoost spec}
xgb_spec <- boost_tree(
  trees = 1000, 
  tree_depth = tune(), min_n = tune(), 
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), mtry = tune(),         ## randomness
  learn_rate = tune(),                         ## step size
) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")
```

```{r create hyperparameter hypercube}
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), combi_train),
  learn_rate(),
  size = 50
)

xgb_grid
```
```{r create workflow}
xgb_wf <- workflow() %>%
  add_formula(imd_2019 ~ .) %>%
  add_model(xgb_spec)

xgb_wf
```
```{r create cross-validation folds}
combi_folds <- vfold_cv(combi_train, strata = imd_2019)

combi_folds
```
```{r}
doParallel::registerDoParallel()

set.seed(235)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = combi_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)

xgb_res
```
```{r}
xgb_res %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "RSQ")
```


```{r}
collect_metrics(xgb_res)
```

```{r}
show_best(xgb_res, "rsq")
best_rsq <- select_best(xgb_res, "rmse")
final_xgb <- finalize_workflow(
  xgb_wf,
  best_rsq
)

final_xgb
```
```{r}
final_res <- last_fit(final_xgb, combi_split)

collect_metrics(final_res)
```
```{r}
final_xgb <- xgb_spec %>% finalize_model(best_rsq)
final_xgb %>%
    fit(formula = imd_2019 ~ ., data = combi_train) %>%
    predict(new_data = combi_test) %>%
    bind_cols(testing(combi_split)) %>%
    yardstick::metrics(imd_2019, .pred) %>%
    mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
```
```{r}
xgb_model <- final_xgb %>%
    fit(formula = imd_2019 ~ ., data = combi_train)
```

```{r}
missing_data <- imd %>% filter(is.na(imd_2019))

xgb_model %>%
    predict(new_data = missing_data) %>%
    bind_cols(missing_data)
```


## Comments

Possible improvements:
- Use lookup tables to bring in all data sources
- Consider using multi-level modeling techniques
- Implement cross-validation to ensure generalisation
- Use regularisation to reduce generalisation gap
- Consider more complex models
- Model using a transformed expenditure
- Use time series modelling
